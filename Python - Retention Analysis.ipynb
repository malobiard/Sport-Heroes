{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliothèques"
      ],
      "metadata": {
        "id": "YqzXNNeZWsPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import files\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RUkhbu8EWr9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import fichiers"
      ],
      "metadata": {
        "id": "pWkvBQvKWtuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btfRPAq_WHGK"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"/content/Data to analyse - 1 month retention.csv\")\n",
        "df1.set_index(\"distinct_id\", inplace = True)\n",
        "df1bis = df1.copy()\n",
        "df1bis = df1bis.drop(df1bis[df1bis['retention 1 month'] == 0].index)\n",
        "df1bis.drop('email', axis=1, inplace=True)\n",
        "df1bis.drop('created', axis=1, inplace=True)\n",
        "df1bis.drop('created_day', axis=1, inplace=True)\n",
        "scaler = StandardScaler()\n",
        "df1_standard = scaler.fit_transform(df1bis.drop(columns=['retention 1 month']))\n",
        "df1_standard = pd.DataFrame(df1_standard, index= df1bis.index, columns=df1bis.columns.drop('retention 1 month'))\n",
        "df1_standard['retention 1 month'] = df1bis['retention 1 month']\n",
        "\n",
        "df3 = pd.read_csv(\"/content/Data to analyse - 3 months retention.csv\")\n",
        "df3.set_index(\"distinct_id\", inplace = True)\n",
        "df3bis = df3.copy()\n",
        "df3bis = df3bis.drop(df3bis[df3bis['Retention 3 months'] == 0].index)\n",
        "df3bis.drop('email', axis=1, inplace=True)\n",
        "df3bis.drop('created', axis=1, inplace=True)\n",
        "scaler = StandardScaler()\n",
        "df3_standard = scaler.fit_transform(df3bis.drop(columns=['Retention 3 months']))\n",
        "df3_standard = pd.DataFrame(df3_standard, index= df3bis.index, columns=df3bis.columns.drop('Retention 3 months'))\n",
        "df3_standard['Retention 3 months'] = df3bis['Retention 3 months']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegLog 1 month - En pause, peut-être utile à l'avenir"
      ],
      "metadata": {
        "id": "eG6W8r6Icps8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# créer un objet de régression logistique\n",
        "model = LogisticRegression()\n",
        "\n",
        "# diviser les données en ensemble d'entraînement et ensemble de test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data1, test_data1, train_target1, test_target1 = train_test_split(df1_standard.iloc[:, :-1], df1_standard.iloc[:, -1], test_size=0.2, random_state=0)\n",
        "\n",
        "# entraîner le modèle sur l'ensemble d'entraînement\n",
        "model.fit(train_data1, train_target1)\n",
        "\n",
        "# prédire les valeurs de l'ensemble de test\n",
        "predictions1 = model.predict(test_data1)\n",
        "\n",
        "# évaluer la performance du modèle\n",
        "print(confusion_matrix(test_target1, predictions1))\n",
        "print(classification_report(test_target1, predictions1))\n",
        "print(accuracy_score(test_target1, predictions1))\n"
      ],
      "metadata": {
        "id": "Sa2UqY_3Xbrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegLog 3 months - En pause, peut-être utile à l'avenir"
      ],
      "metadata": {
        "id": "Gjj5pvbifHYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# créer un objet de régression logistique\n",
        "model = LogisticRegression()\n",
        "\n",
        "# diviser les données en ensemble d'entraînement et ensemble de test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data3, test_data3, train_target3, test_target3 = train_test_split(df3_standard.iloc[:, :-1], df3_standard.iloc[:, -1], test_size=0.2, random_state=0)\n",
        "\n",
        "# entraîner le modèle sur l'ensemble d'entraînement\n",
        "model.fit(train_data3, train_target3)\n",
        "\n",
        "# prédire les valeurs de l'ensemble de test\n",
        "predictions3 = model.predict(test_data3)\n",
        "\n",
        "# évaluer la performance du modèle\n",
        "print(confusion_matrix(test_target3, predictions3))\n",
        "print(classification_report(test_target3, predictions3))\n",
        "print(accuracy_score(test_target3, predictions3))"
      ],
      "metadata": {
        "id": "ON4Dq0w_fHge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KBest 1 month"
      ],
      "metadata": {
        "id": "9wih55fVgJTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The KBest method is used in machine learning to select the best variables to predict a target variable.**\n",
        "\n",
        "In our case, we want to predict retention, which is our target variable. This method uses several indicators to measure the correlation between the variables and the target.\n",
        "\n",
        "Since all our variables are numeric, the KBest method will apply an analysis of variance method (**f_classif**). It measures the variance between groups of values and calculates an F score that measures the difference between groups. The higher the score, the more important the variable is considered for predicting the target variable.\n",
        "\n",
        "Once this score is calculated for each variable, we can determine which ones have the most influence on the prediction."
      ],
      "metadata": {
        "id": "p8LsRLSOyuh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1_standard.drop('retention 1 month', axis=1)\n",
        "y = df1_standard['retention 1 month']\n",
        "k_best = SelectKBest(f_classif, k = 'all')\n",
        "k_best.fit(X, y)\n",
        "scores1 = pd.DataFrame({'Variable': X.columns, 'Score': k_best.scores_})\n",
        "scores1 = scores1.sort_values('Score', ascending=False)\n",
        "print(scores1)"
      ],
      "metadata": {
        "id": "0UVE6IZ4gJcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to this analysis using the KBest method, we can consider \"Leaderboard related pageview\", \"challenge entered (7 days)\", \"Challenge completed (7 days)\" and \"Activity != Walking (7 days)\" (in this order) as the most important variables in predicting retention."
      ],
      "metadata": {
        "id": "TDD3MfefzQEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KBest 3 months"
      ],
      "metadata": {
        "id": "9ylyhIIqgJlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df3_standard.drop('Retention 3 months', axis=1)\n",
        "y = df3_standard['Retention 3 months']\n",
        "k_best = SelectKBest(f_classif, k='all')\n",
        "k_best.fit(X, y)\n",
        "scores3 = pd.DataFrame({'Variable': X.columns, 'Score': k_best.scores_})\n",
        "scores3 = scores3.sort_values('Score', ascending=False)\n",
        "print(scores3)"
      ],
      "metadata": {
        "id": "C1VZg58agJts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "yqRimL4mA-x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PCA algorithm works by finding the directions of maximum variance in the original dataset and projecting the data onto these new axes. The first principal component is the direction with the highest variance, followed by the second principal component, which is orthogonal to the first and has the second highest variance, and so on. Each principal component is a linear combination of the original features, and together they account for the total variance in the data.\n",
        "\n",
        "Once the principal components have been computed, the number of components to retain can be selected based on the amount of variance that they explain. This can be done by looking at the eigenvalues of the covariance matrix, which represent the amount of variance in each principal component. Components with high eigenvalues are considered to be more important and are retained, while components with low eigenvalues can be discarded."
      ],
      "metadata": {
        "id": "TLrX6CLx4lWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1_standard.drop('retention 1 month', axis=1)\n",
        "y = df1_standard['retention 1 month']\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "pd.DataFrame(pca.components_, columns=X.columns, index=['PC1', 'PC2'])"
      ],
      "metadata": {
        "id": "HY2lzWAzA-5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to this PCA, we can affirm that the variables with the most impact on the total variance, and therefore the most significant ones. In other words, they are the ones whose variation will have the most impact on the variation of the others. Indeed, the coefficients above correspond to the respective contribution of each variable to the two principal components (aX1 + bX2 + etc...). The largest coefficients in absolute value are therefore those of the most important variables, regardless of whether we consider PC1 or PC2. So for example if a variable has a large coefficient for PC1 and PC2, it is very important. If another has a strong component for PC1 and a weak one for PC2, it is important, as much as if PC1 and PC2 were reversed.\n",
        "\n",
        "The goal is that we can therefore extract the same variables as before as being predominant: \"Leaderboard related pageview\", \"challenge entered (7 days)\", \"Challenge completed (7 days)\" and \"Activity != Walking (7 days)\"\n",
        "\n",
        "However, these results are not perfect, because although these variables are the most significant, the differences with the other variables remain quite small."
      ],
      "metadata": {
        "id": "E5Ose3Q05QMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrélations 1 month"
      ],
      "metadata": {
        "id": "vZ5qh23L8ldS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlations1 = df1bis.corr(method = 'pearson')[\"retention 1 month\"]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(correlations1)"
      ],
      "metadata": {
        "id": "EKGPTktS8llh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the .corr() function, we use a basic function from the pandas library to measure the correlation between our variables and the target. This function is similar to the KBest method, but uses the Pearson correlation coefficient. The latter is suitable for continuous variables, but is also relevant in our case, with numeric variables. Once again, the results are similar to the previous ones: \"Leaderboard related pageview\", \"challenge entered (7 days)\", \"Challenge completed (7 days)\", and \"Activity != Walking (7 days)\" are the variables most correlated to the target according to this method."
      ],
      "metadata": {
        "id": "5FEk3ZGPMOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest method\n",
        "# Split the data into X (features) and y (target variable)\n",
        "X1bis = df1bis.drop(\"retention 1 month\", axis=1)\n",
        "y1bis = df1bis[\"retention 1 month\"]\n",
        "\n",
        "# Train a Random Forest regressor model\n",
        "rf1 = RandomForestRegressor()\n",
        "rf1.fit(X1bis, y1bis)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances1 = pd.DataFrame(rf1.feature_importances_,\n",
        "                                   index = X1bis.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(feature_importances1)"
      ],
      "metadata": {
        "id": "irJHxb5MBgXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"**feature_importances_**\" function calculates the relative importance of each variable in the model, that is, the contribution of each variable to the model's accuracy. It estimates the importance of each column by computing the reduction in average impurity (measured by the Gini index or entropy) obtained when using that variable to split the decision trees in the ensemble. More specifically, the function **calculates the average reduction in impurity weighted by the number of observations in each node for each feature**."
      ],
      "metadata": {
        "id": "WlQtzNdpB58O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrélations 3 months"
      ],
      "metadata": {
        "id": "SdtUsEbU8lra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlations3 = df3bis.corr()[\"Retention 3 months\"]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(correlations3)"
      ],
      "metadata": {
        "id": "0xgNEzuy8lxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest method\n",
        "# Split the data into X (features) and y (target variable)\n",
        "X3bis = df3bis.drop(\"Retention 3 months\", axis=1)\n",
        "y3bis = df3bis[\"Retention 3 months\"]\n",
        "\n",
        "# Train a Random Forest regressor model\n",
        "rf3 = RandomForestRegressor()\n",
        "rf3.fit(X3bis, y3bis)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances3 = pd.DataFrame(rf3.feature_importances_,\n",
        "                                   index = X3bis.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(feature_importances3)"
      ],
      "metadata": {
        "id": "cmF7ypYO-cM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Regression 1 month"
      ],
      "metadata": {
        "id": "HKgbKVQEJHu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xmv1 = df1_standard.drop(['retention 1 month'], axis=1)\n",
        "ymv1 = df1_standard['retention 1 month']\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = LinearRegression()\n",
        "\n",
        "# Entrainement du modèle\n",
        "model.fit(Xmv1, ymv1)\n",
        "\n",
        "# Affichage des coefficients de régression\n",
        "coefficients = pd.DataFrame({\n",
        "    'Variable': Xmv1.columns,\n",
        "    'Coefficients': model.coef_\n",
        "})\n",
        "print(coefficients.sort_values(by = 'Coefficients', ascending = False))"
      ],
      "metadata": {
        "id": "txaO6cLNLTGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have created a 'multivariate regression' model, in order to highlight the importance of each variable in the prediction of the target. The regression model used returns a function of the form: y = aX1 + bX2 + ... + zXn. Each Xi corresponds to our different variables, and the coefficient preceding them to their importance in the prediction model. Thus, we obtain for each variable its importance. As in the case of the other models, we find a strong predominance of \"Leaderboard related pageview\", \"Challenge entered (7 days)\", \"Challenge completed (7 days)\" and \"Activity != Walking (7 days)\"."
      ],
      "metadata": {
        "id": "LdCdASTPL1jG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-means clustering 1 month"
      ],
      "metadata": {
        "id": "Vms_RR_NNiKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1cluster = df1.copy()\n",
        "df1cluster.drop('email', axis=1, inplace=True)\n",
        "df1cluster.drop('created', axis=1, inplace=True)\n",
        "df1cluster.drop('created_day', axis=1, inplace=True)\n",
        "df1cluster = df1cluster.drop(df1cluster[df1cluster['retention 1 month'] == 0].index)\n",
        "df1cluster_test = df1cluster.copy()\n",
        "challE_avg = df1cluster['challenge entered (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['challenge entered (7 days)'] >= challE_avg, 'challenge entered (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['challenge entered (7 days)'] < challE_avg, 'challenge entered (7 days)'] = 0\n",
        "challC_avg = df1cluster['challenge completed (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['challenge completed (7 days)'] >= challC_avg, 'challenge completed (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['challenge completed (7 days)'] < challC_avg, 'challenge completed (7 days)'] = 0\n",
        "article_avg = df1cluster['article read (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['article read (7 days)'] >= article_avg, 'article read (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['article read (7 days)'] < article_avg, 'article read (7 days)'] = 0\n",
        "interaction_avg = df1cluster['interaction created (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['interaction created (7 days)'] >= interaction_avg, 'interaction created (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['interaction created (7 days)'] < interaction_avg, 'interaction created (7 days)'] = 0\n",
        "leaderboard_avg = df1cluster['leaderboard related pageview'].median()\n",
        "df1cluster_test.loc[df1cluster_test['leaderboard related pageview'] >= leaderboard_avg, 'leaderboard related pageview'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['leaderboard related pageview'] < leaderboard_avg, 'leaderboard related pageview'] = 0\n",
        "activity_avg = df1cluster['activity != Walking (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['activity != Walking (7 days)'] >= activity_avg, 'activity != Walking (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['activity != Walking (7 days)'] < activity_avg, 'activity != Walking (7 days)'] = 0\n",
        "track_avg = df1cluster['tracking app (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['tracking app (7 days)'] >= 1, 'tracking app (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['tracking app (7 days)'] < 1, 'tracking app (7 days)'] = 0\n",
        "club_avg = df1cluster['club joined (7 days)'].median()\n",
        "df1cluster_test.loc[df1cluster_test['club joined (7 days)'] >= club_avg, 'club joined (7 days)'] = 1\n",
        "df1cluster_test.loc[df1cluster_test['club joined (7 days)'] < club_avg, 'club joined (7 days)'] = 0\n",
        "scaler = StandardScaler()\n",
        "df1_standard_cluster = scaler.fit_transform(df1cluster_test.drop(columns=['retention 1 month']))\n",
        "df1_standard_cluster = pd.DataFrame(df1_standard_cluster, index= df1cluster_test.index, columns=df1cluster_test.columns.drop('retention 1 month'))\n",
        "df1_standard_cluster['retention 1 month'] = df1cluster_test['retention 1 month']\n",
        "print(df1cluster_test)"
      ],
      "metadata": {
        "id": "NQHkJSAwe2_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Methode du coude\n",
        "# Extraction des données de la dataframe\n",
        "X = df1_standard.values\n",
        "\n",
        "# Normalisation des données\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Liste des valeurs de K à tester\n",
        "K = range(1, 10)\n",
        "\n",
        "# Liste des valeurs des variances intra-cluster pour chaque K\n",
        "inertias = []\n",
        "\n",
        "for k in K:\n",
        "    # Créer un modèle de KMeans pour chaque K\n",
        "    model = KMeans(n_clusters=k)\n",
        "\n",
        "    # Ajuster le modèle aux données\n",
        "    model.fit(X)\n",
        "\n",
        "    # Ajouter l'inertie (variance intra-cluster) à la liste\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "# Tracer le graphique de la méthode du coude\n",
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('Nombre de clusters (K)')\n",
        "plt.ylabel('Variance intra-cluster')\n",
        "plt.title(\"Méthode du coude pour déterminer le nombre optimal de clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TX5XNgAUNibD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Clustering\n",
        "# Choisir le nombre de clusters (k)\n",
        "k = 3\n",
        "\n",
        "# Sélectionner les colonnes sur lesquelles faire le clustering\n",
        "cols = [df1_standard.columns[8]]\n",
        "\n",
        "# Sélectionner les données pour le clustering\n",
        "data = df1_standard[cols]\n",
        "\n",
        "# Initialiser le modèle de clustering\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "\n",
        "# Effectuer le clustering\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Ajouter une colonne \"cluster\" à votre tableau de données\n",
        "df1bis[\"retention1_cluster\"] = kmeans.labels_\n",
        "\n",
        "# Afficher le tableau de données avec la colonne \"cluster\"\n",
        "stats = df1bis.groupby('retention1_cluster').agg(['mean'])#, 'median', 'var', 'std'])\n",
        "print(stats)\n",
        "#stats.to_csv('stats_retention_cluster.csv', index=True)\n",
        "#files.download('stats_retention_cluster.csv')\n",
        "#df1cluster.head()\n",
        "comptage = df1bis['retention1_cluster'].value_counts()\n",
        "comptage"
      ],
      "metadata": {
        "id": "nNy7c7gJO5Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1cluster = df1.copy()\n",
        "df1cluster.drop('email', axis=1, inplace=True)\n",
        "df1cluster.drop('created', axis=1, inplace=True)\n",
        "df1cluster.drop('created_day', axis=1, inplace=True)\n",
        "df1cluster = df1cluster.drop(df1cluster[df1cluster['retention 1 month'] == 0].index)\n",
        "df1cluster = df1cluster.drop(df1cluster[df1cluster['challenge entered (7 days)'] > 2].index)\n",
        "df1cluster = df1cluster.drop(df1cluster[df1cluster['leaderboard related pageview'] > 2].index)\n",
        "scaler = StandardScaler()\n",
        "df1_standard_cluster = scaler.fit_transform(df1cluster.drop(columns=['retention 1 month']))\n",
        "df1_standard_cluster = pd.DataFrame(df1_standard_cluster, index= df1cluster.index, columns=df1cluster.columns.drop('retention 1 month'))\n",
        "df1_standard_cluster['retention 1 month'] = df1cluster['retention 1 month']\n",
        "#print(df1cluster_test)\n",
        "\n",
        "##Clustering\n",
        "# Choisir le nombre de clusters (k)\n",
        "k = 4\n",
        "\n",
        "# Sélectionner les colonnes sur lesquelles faire le clustering\n",
        "cols = [df1_standard_cluster.columns[2],df1_standard_cluster.columns[3],df1_standard_cluster.columns[5],df1_standard_cluster.columns[7]]\n",
        "\n",
        "# Sélectionner les données pour le clustering\n",
        "data = df1_standard_cluster[cols]\n",
        "\n",
        "# Initialiser le modèle de clustering\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "\n",
        "# Effectuer le clustering\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Ajouter une colonne \"cluster\" à votre tableau de données\n",
        "df1cluster[\"retention1_cluster\"] = kmeans.labels_\n",
        "\n",
        "# Afficher le tableau de données avec la colonne \"cluster\"\n",
        "stats = df1cluster.groupby('retention1_cluster').agg(['mean'])#, 'median', 'var', 'std'])\n",
        "print(stats)\n",
        "stats.to_csv('stats_retention_cluster.csv', index=True)\n",
        "#files.download('stats_retention_cluster.csv')\n",
        "#df1cluster.head()\n",
        "comptage = df1cluster['retention1_cluster'].value_counts()\n",
        "comptage\n",
        "#stats.to_csv('stats_retention_cluster.csv', index=True)\n",
        "#files.download('stats_retention_cluster.csv')"
      ],
      "metadata": {
        "id": "6zkMr7EgpTEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Définir le nombre de clusters\n",
        "k = 3\n",
        "\n",
        "# Instancier le modèle de clustering spectral\n",
        "model = SpectralClustering(n_clusters=k, affinity='nearest_neighbors')\n",
        "\n",
        "# Effectuer le clustering sur les données\n",
        "clusters = model.fit_predict(df1_standard)\n",
        "\n",
        "# Ajouter les résultats du clustering à votre dataframe\n",
        "df1bis['cluster'] = clusters\n",
        "\n",
        "stats = df1bis.groupby('cluster').agg(['mean'])\n",
        "print(stats)\n",
        "comptage = df1bis['cluster'].value_counts()\n",
        "print(comptage)"
      ],
      "metadata": {
        "id": "2nzDQRyugoGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "# Sélectionner les colonnes sur lesquelles vous souhaitez effectuer le clustering\n",
        "data = df1_standard.iloc[:, :-1].values\n",
        "\n",
        "# Calculer la matrice de distance entre les observations\n",
        "dist_matrix = linkage(data, method='ward')\n",
        "\n",
        "# Définir le nombre de clusters\n",
        "k = 6\n",
        "\n",
        "# Assigner chaque observation à un cluster\n",
        "clusters = fcluster(dist_matrix, k, criterion='maxclust')\n",
        "\n",
        "# Ajouter les résultats du clustering à votre dataframe\n",
        "df1bis['cluster'] = clusters\n",
        "stats = df1bis.groupby('cluster').agg(['mean'])\n",
        "print(stats)\n",
        "comptage = df1bis['cluster'].value_counts()\n",
        "print(comptage)"
      ],
      "metadata": {
        "id": "3FjtwQhSjG3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WoE & IV"
      ],
      "metadata": {
        "id": "NFNd_aXOGZDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code allows us to measure several indicators: First the weight of evidence (WoE) intra-variable, the WoE on whole variables, and the information value (IV).\n",
        "\n",
        "**Let's start with the within-variable WoE**. The algorithm will split each variable into equiprobable segments, i.e. they will have approximately the same number of users. Then for each of these segments, we will compute the logarithm of the ratio between active users after one month in this segment, and those not active after one month. We weight this ratio so that in the end, the logarithm is positive when 70% of the users in the segment are active after one month. This allows us to set threshold values for each variable, which allow us to identify for which values at least 70% of the users remain active after one month.\n",
        "\n",
        "**The total WoE** is the sum of the WoE of each segment of a variable. It is not very valuable because it does not give any information about the threshold values we are looking for. Nevertheless, it allows us to have an idea of the importance of a variable, but the measure is less relevant here than the ones performed previously.\n",
        "\n",
        "**It is the same case for the IV**, which we will not exploit either."
      ],
      "metadata": {
        "id": "FohvYEWfnsWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "df100 = df1bis.copy()\n",
        "df100.iloc[:, -1] = df100.iloc[:, -1].apply(lambda x: 1 if x > 0 else x)\n",
        "df100bis = df100.copy()\n",
        "\n",
        "# Définir la variable cible et les variables prédictives\n",
        "target = 'retention 1 month'\n",
        "predictors = list(df100.columns[:-1])\n",
        "\n",
        "# Calculer le WoE et l'IV pour chaque variable prédictive\n",
        "def woe_iv(df, var, target, bins=30):\n",
        "    \"\"\"\n",
        "    Calcule le WoE et l'IV pour une variable donnée\n",
        "    \"\"\"\n",
        "    # Créer des intervalles de valeur pour la variable\n",
        "    cut_points = pd.qcut(df[var], q=bins, duplicates = 'drop', retbins=True)[1]\n",
        "    if len(cut_points) < 2:\n",
        "        cut_points = [-np.inf, np.inf]\n",
        "    labels = range(len(cut_points) - 1)\n",
        "\n",
        "    # Créer une variable discrète avec les intervalles de valeur\n",
        "    df[var + '_bin'] = pd.cut(df[var], bins=cut_points, labels=labels, include_lowest=True)\n",
        "\n",
        "    # Calculer les proportions de cas positifs et négatifs pour chaque intervalle de valeur\n",
        "    pos_df = df[df[target]==1].groupby(var + '_bin').size().reset_index(name='pos')\n",
        "    neg_df = df[df[target]==0].groupby(var + '_bin').size().reset_index(name='neg')\n",
        "    pos_df = pos_df.merge(neg_df, on=var + '_bin', how='left', suffixes=('_pos', '_neg'))\n",
        "\n",
        "    # Remplacer les valeurs manquantes par 0.5 pour éviter les erreurs de calcul\n",
        "    #pos_df.fillna(0.5, inplace = True)\n",
        "\n",
        "    # Ajouter une constante pour éviter les valeurs infinies\n",
        "    pos_df['pos'] = pos_df['pos'] + 0.5\n",
        "    pos_df['neg'] = pos_df['neg'] + 0.5\n",
        "\n",
        "    # Calculer les proportions totales de cas positifs et négatifs\n",
        "    pos_total = pos_df['pos'].sum()\n",
        "    neg_total = pos_df['neg'].sum()\n",
        "\n",
        "    # Calculer le WoE et l'IV pour chaque intervalle de valeur\n",
        "    pos_df['woe'] = np.log(pos_df['pos']/pos_df['neg']) - np.log(7/3)# - np.log(pos_df['neg']/neg_total)\n",
        "    pos_df['iv'] = pos_df['woe'] * (pos_df['pos']/pos_total - pos_df['neg']/neg_total)\n",
        "\n",
        "    # Calculer le WoE global et l'IV global pour la variable\n",
        "    woe = pos_df['woe'].sum()\n",
        "    iv = pos_df['iv'].sum()\n",
        "\n",
        "    # Créer un dictionnaire avec le WoE pour chaque intervalle de valeur\n",
        "    woe_dict = {}\n",
        "    for i, cut in enumerate(cut_points[:-1]):\n",
        "        label = labels[i]\n",
        "        woe_dict[str(cut) + '-' + str(cut_points[i+1])] = pos_df.loc[pos_df[var + '_bin']==label, 'woe'].iloc[0]\n",
        "\n",
        "    return woe, iv, woe_dict\n",
        "\n",
        "woe_dictbis = {}\n",
        "woe_dict = {}\n",
        "iv_dict = {}\n",
        "\n",
        "for var in predictors:\n",
        "    woe, iv, woe_dicti = woe_iv(df100, var, target)\n",
        "    woe_dict[var] = woe_dicti\n",
        "    woe_dictbis[var] = woe\n",
        "    iv_dict[var] = iv\n",
        "\n",
        "# Afficher les résultats\n",
        "print('WoE chaque feature:')\n",
        "for k, v in woe_dict.items():\n",
        "    print(k, v)\n",
        "    print(len(v))\n",
        "print('WoE:')\n",
        "for k, v in woe_dictbis.items():\n",
        "    print(k, v)\n",
        "\n",
        "print('IV:')\n",
        "for k, v in iv_dict.items():\n",
        "    print(k, v)"
      ],
      "metadata": {
        "id": "5b2bjQSaN25p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we find an algorithm that for each variable, calculates the minimum value from which 60% or more of the users with a value (for this variable) greater than or equal to the minimum value are still active after one month."
      ],
      "metadata": {
        "id": "HJsK8H--5Vv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultats = {}\n",
        "\n",
        "# Pour chaque colonne numérique sauf la colonne cible\n",
        "for nom_colonne in df100bis.columns:\n",
        "    if nom_colonne != 'retention 1 month':\n",
        "        # Tri des valeurs de la colonne par ordre croissant\n",
        "        valeurs_triees = sorted(df100bis[nom_colonne].unique())\n",
        "\n",
        "        # Pour chaque valeur triée\n",
        "        for valeur in valeurs_triees:\n",
        "            # Sélection des lignes ayant une valeur supérieure ou égale à cette valeur\n",
        "            lignes_selectionnees = df100bis[df100bis[nom_colonne] >= valeur]\n",
        "\n",
        "            # Calcul du pourcentage de lignes avec la valeur 1 dans la colonne cible\n",
        "            pourcentage = lignes_selectionnees['Retention 1 month'].mean() * 100\n",
        "\n",
        "            # Si le pourcentage est supérieur ou égal à 60%\n",
        "            if pourcentage >= 50:\n",
        "                # Stockage de la valeur minimale pour cette colonne\n",
        "                resultats[nom_colonne] = valeur\n",
        "                break\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"To reach 60% of active users after 1 month :\")\n",
        "for key, value in resultats.items():\n",
        "    print(\"minimum amount of \" + key + \": \" +str(value))"
      ],
      "metadata": {
        "id": "Fs2unOhU1kCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}